{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "867c214e-e409-4a18-ab69-17d51d2ff5a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task ID</th>\n",
       "      <th>Priority</th>\n",
       "      <th>Latency</th>\n",
       "      <th>Processing Requirements</th>\n",
       "      <th>Execution Node</th>\n",
       "      <th>Actual Execution Time (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "      <td>Low</td>\n",
       "      <td>High</td>\n",
       "      <td>Fog</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Cloud</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Low</td>\n",
       "      <td>High</td>\n",
       "      <td>Low</td>\n",
       "      <td>Fog</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>High</td>\n",
       "      <td>Low</td>\n",
       "      <td>High</td>\n",
       "      <td>Fog</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Low</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Cloud</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Medium</td>\n",
       "      <td>High</td>\n",
       "      <td>Low</td>\n",
       "      <td>Cloud</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>High</td>\n",
       "      <td>Low</td>\n",
       "      <td>High</td>\n",
       "      <td>Fog</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Low</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Cloud</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Medium</td>\n",
       "      <td>High</td>\n",
       "      <td>Low</td>\n",
       "      <td>Fog</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>High</td>\n",
       "      <td>Low</td>\n",
       "      <td>High</td>\n",
       "      <td>Fog</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Task ID Priority Latency Processing Requirements Execution Node   \n",
       "0        1     High     Low                    High            Fog  \\\n",
       "1        2   Medium  Medium                  Medium          Cloud   \n",
       "2        3      Low    High                     Low            Fog   \n",
       "3        4     High     Low                    High            Fog   \n",
       "4        5      Low  Medium                  Medium          Cloud   \n",
       "5        6   Medium    High                     Low          Cloud   \n",
       "6        7     High     Low                    High            Fog   \n",
       "7        8      Low  Medium                  Medium          Cloud   \n",
       "8        9   Medium    High                     Low            Fog   \n",
       "9       10     High     Low                    High            Fog   \n",
       "\n",
       "   Actual Execution Time (ms)  \n",
       "0                          50  \n",
       "1                         200  \n",
       "2                          30  \n",
       "3                          60  \n",
       "4                         150  \n",
       "5                         120  \n",
       "6                          55  \n",
       "7                         180  \n",
       "8                          40  \n",
       "9                          70  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "   \n",
    "df = pd.read_csv('E:\\Task scheduling project\\priority.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "727d7db2-1ea2-46ad-9267-6b1e7674a106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'Execution Node' column: ['Fog' 'Cloud']\n",
      "Entropy of the entire dataset: 0.9710\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('E:\\\\Task scheduling project\\\\priority.csv')\n",
    "\n",
    "# Check the unique values in the 'Execution Node' column\n",
    "print(\"Unique values in 'Execution Node' column:\", df['Execution Node'].unique())\n",
    "\n",
    "# Calculate the probabilities for each class\n",
    "class_counts = df['Execution Node'].value_counts()\n",
    "total_samples = len(df)\n",
    "probabilities = class_counts / total_samples\n",
    "\n",
    "# Ensure probabilities sum to 1\n",
    "probabilities_sum = probabilities.sum()\n",
    "if probabilities_sum != 1:\n",
    "    probabilities /= probabilities_sum\n",
    "\n",
    "# Calculate entropy\n",
    "initial_entropy = -sum(prob * math.log2(prob) for prob in probabilities)\n",
    "\n",
    "print(f\"Entropy of the entire dataset: {initial_entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "79da0f3c-5797-43f2-b6b5-0176feef8ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of 'Execution Node' for 'Priority' High: -0.0000\n",
      "Entropy of 'Execution Node' for 'Priority' Medium: 0.9183\n",
      "Entropy of 'Execution Node' for 'Priority' Low: 0.9183\n"
     ]
    }
   ],
   "source": [
    "unique_priorities = df['Priority'].unique()\n",
    "\n",
    "for priority_value in unique_priorities:\n",
    "    subset = df[df['Priority'] == priority_value]\n",
    "    subset_size = len(subset)\n",
    "    \n",
    "    # Calculate the probabilities for each class within the subset\n",
    "    class_counts = subset['Execution Node'].value_counts()\n",
    "    probabilities = class_counts / subset_size\n",
    "    \n",
    "    # Calculate entropy for the current priority value\n",
    "    entropy = -sum(prob * math.log2(prob) for prob in probabilities)\n",
    "    \n",
    "    print(f\"Entropy of 'Execution Node' for 'Priority' {priority_value}: {entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "52124cbc-aca1-4bf0-99f2-f2be617bc04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain for 'Priority': 0.4200\n"
     ]
    }
   ],
   "source": [
    "# Calculate the entropy of the entire dataset\n",
    "def calculate_entropy(data):\n",
    "    class_counts = data['Execution Node'].value_counts()\n",
    "    probabilities = class_counts / len(data)\n",
    "    entropy = -sum(prob * math.log2(prob) for prob in probabilities)\n",
    "    return entropy\n",
    "\n",
    "initial_entropy = calculate_entropy(df)\n",
    "\n",
    "# Calculate the information gain for the \"Priority\" attribute\n",
    "def calculate_information_gain(data, attribute):\n",
    "    unique_values = data[attribute].unique()\n",
    "    weighted_entropy_after_split = 0\n",
    "\n",
    "    for value in unique_values:\n",
    "        subset = data[data[attribute] == value]\n",
    "        subset_weight = len(subset) / len(data)\n",
    "        weighted_entropy_after_split += subset_weight * calculate_entropy(subset)\n",
    "\n",
    "    information_gain = initial_entropy - weighted_entropy_after_split\n",
    "    return information_gain\n",
    "\n",
    "# Calculate information gain for the \"Priority\" attribute\n",
    "information_gain_priority = calculate_information_gain(df, 'Priority')\n",
    "\n",
    "print(f\"Information Gain for 'Priority': {information_gain_priority:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eae99044-882c-4c93-9640-0ce0f8af77d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6954618442383218"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate information gain for the \"Latency\" attribute\n",
    "information_gain_latency = calculate_information_gain(df, 'Latency')\n",
    "information_gain_latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c7742882-7cf6-4c4a-8bba-632c0fc0e0c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6954618442383218"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_gain_processing_requirements = calculate_information_gain(df, 'Processing Requirements')\n",
    "information_gain_processing_requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0afabb3a-4123-408f-a93d-0219fd885742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4199730940219749\n",
      "0.6954618442383218\n",
      "0.6954618442383218\n"
     ]
    }
   ],
   "source": [
    "print(information_gain_priority)\n",
    "print(information_gain_latency)\n",
    "print(information_gain_processing_requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "899e8b98-0458-4de0-9708-ee5816c91ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset for Latency = Low:\n",
      "   Task ID Priority Latency Processing Requirements Execution Node   \n",
      "0        1     High     Low                    High            Fog  \\\n",
      "3        4     High     Low                    High            Fog   \n",
      "6        7     High     Low                    High            Fog   \n",
      "9       10     High     Low                    High            Fog   \n",
      "\n",
      "   Actual Execution Time (ms)  \n",
      "0                          50  \n",
      "3                          60  \n",
      "6                          55  \n",
      "9                          70  \n",
      "\n",
      "Subset for Latency = Medium:\n",
      "   Task ID Priority Latency Processing Requirements Execution Node   \n",
      "1        2   Medium  Medium                  Medium          Cloud  \\\n",
      "4        5      Low  Medium                  Medium          Cloud   \n",
      "7        8      Low  Medium                  Medium          Cloud   \n",
      "\n",
      "   Actual Execution Time (ms)  \n",
      "1                         200  \n",
      "4                         150  \n",
      "7                         180  \n",
      "\n",
      "Subset for Latency = High:\n",
      "   Task ID Priority Latency Processing Requirements Execution Node   \n",
      "2        3      Low    High                     Low            Fog  \\\n",
      "5        6   Medium    High                     Low          Cloud   \n",
      "8        9   Medium    High                     Low            Fog   \n",
      "\n",
      "   Actual Execution Time (ms)  \n",
      "2                          30  \n",
      "5                         120  \n",
      "8                          40  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose the attribute with the highest information gain as the splitting criterion\n",
    "selected_attribute = 'Latency'\n",
    "\n",
    "# Get unique values of the selected attribute\n",
    "unique_values = df[selected_attribute].unique()\n",
    "\n",
    "# Create a dictionary to store subsets\n",
    "subsets = {}\n",
    "\n",
    "# Split the dataset into subsets based on the selected attribute\n",
    "for value in unique_values:\n",
    "    subsets[value] = df[df[selected_attribute] == value]\n",
    "\n",
    "# Print the subsets\n",
    "for value, subset in subsets.items():\n",
    "    print(f\"Subset for {selected_attribute} = {value}:\\n{subset}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9a3d58d1-12eb-4e82-b9f2-aa1e00527c34",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Execution Node'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m attributes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPriority\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatency\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing Requirements\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     73\u001b[0m target_attribute \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExecution Node\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Update this if the actual column name is different\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m decision_tree \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_decision_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_attribute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_samples_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Print the decision tree (you might want to use a tree visualization library for a more readable display)\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(decision_tree)\n",
      "Cell \u001b[1;32mIn[70], line 56\u001b[0m, in \u001b[0;36mcreate_decision_tree\u001b[1;34m(data, attributes, target_attribute, max_depth, min_samples_split)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: determine_class(data[target_attribute])}\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Determine the best attribute for splitting\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m best_attribute \u001b[38;5;241m=\u001b[39m \u001b[43mdetermine_best_attribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_attribute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Create a decision tree node\u001b[39;00m\n\u001b[0;32m     59\u001b[0m decision_tree \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattribute\u001b[39m\u001b[38;5;124m'\u001b[39m: best_attribute, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubsets\u001b[39m\u001b[38;5;124m'\u001b[39m: {}}\n",
      "Cell \u001b[1;32mIn[70], line 18\u001b[0m, in \u001b[0;36mdetermine_best_attribute\u001b[1;34m(data, attributes, target_attribute)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetermine_best_attribute\u001b[39m(data, attributes, target_attribute):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Calculate the initial entropy of the entire dataset\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     initial_entropy \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_attribute\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Initialize variables to track the best attribute and its information gain\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     best_attribute \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[64], line 3\u001b[0m, in \u001b[0;36mcalculate_entropy\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_entropy\u001b[39m(data):\n\u001b[1;32m----> 3\u001b[0m     class_counts \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExecution Node\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[0;32m      4\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m class_counts \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[0;32m      5\u001b[0m     entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28msum\u001b[39m(prob \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog2(prob) \u001b[38;5;28;01mfor\u001b[39;00m prob \u001b[38;5;129;01min\u001b[39;00m probabilities)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:1007\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1004\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1012\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:1116\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1115\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1116\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:349\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Execution Node'"
     ]
    }
   ],
   "source": [
    "def calculate_info_gain(data, attribute, target_attribute):\n",
    "    # Calculate the initial entropy of the entire dataset\n",
    "    initial_entropy = calculate_entropy(data[target_attribute])\n",
    "\n",
    "    # Calculate information gain for the current attribute\n",
    "    weighted_entropy_after_split = 0\n",
    "\n",
    "    for value in data[attribute].unique():\n",
    "        subset = data[data[attribute] == value]\n",
    "        subset_weight = len(subset) / len(data)\n",
    "        weighted_entropy_after_split += subset_weight * calculate_entropy(subset[target_attribute])\n",
    "\n",
    "    information_gain = initial_entropy - weighted_entropy_after_split\n",
    "    return information_gain\n",
    "\n",
    "def determine_best_attribute(data, attributes, target_attribute):\n",
    "    # Calculate the initial entropy of the entire dataset\n",
    "    initial_entropy = calculate_entropy(data[target_attribute])\n",
    "\n",
    "    # Initialize variables to track the best attribute and its information gain\n",
    "    best_attribute = None\n",
    "    max_information_gain = -1  # Initialize with a value lower than possible information gains\n",
    "\n",
    "    # Iterate over each attribute and calculate information gain\n",
    "    for attribute in attributes:\n",
    "        if attribute == target_attribute:\n",
    "            continue  # Skip the target attribute\n",
    "\n",
    "        # Calculate information gain for the current attribute\n",
    "        information_gain = calculate_info_gain(data, attribute, target_attribute)\n",
    "\n",
    "        # Update the best attribute if the information gain is higher\n",
    "        if information_gain > max_information_gain:\n",
    "            max_information_gain = information_gain\n",
    "            best_attribute = attribute\n",
    "\n",
    "    return best_attribute\n",
    "\n",
    "def determine_class(target_values):\n",
    "    # Count the occurrences of each class in target_values\n",
    "    class_counts = target_values.value_counts()\n",
    "\n",
    "    # Determine the majority class (class with the highest count)\n",
    "    majority_class = class_counts.idxmax()\n",
    "\n",
    "    return majority_class\n",
    "\n",
    "def create_decision_tree(data, attributes, target_attribute, max_depth=None, min_samples_split=2):\n",
    "    # Check stopping criteria\n",
    "    if max_depth is not None and max_depth == 0:\n",
    "        return {'class': determine_class(data[target_attribute])}\n",
    "    if len(data) < min_samples_split:\n",
    "        return {'class': determine_class(data[target_attribute])}\n",
    "\n",
    "    # Determine the best attribute for splitting\n",
    "    best_attribute = determine_best_attribute(data, attributes, target_attribute)\n",
    "\n",
    "    # Create a decision tree node\n",
    "    decision_tree = {'attribute': best_attribute, 'subsets': {}}\n",
    "\n",
    "    # Recursively create decision tree for each subset\n",
    "    for value in data[best_attribute].unique():\n",
    "        subset = data[data[best_attribute] == value]\n",
    "        decision_tree['subsets'][value] = create_decision_tree(subset, attributes, target_attribute,\n",
    "                                                                max_depth=max_depth - 1 if max_depth is not None else None,\n",
    "                                                                min_samples_split=min_samples_split)\n",
    "\n",
    "    return decision_tree\n",
    "\n",
    "# Example usage:\n",
    "# Assuming df is the original dataframe and 'Execution Node' is the target attribute\n",
    "attributes = ['Priority', 'Latency', 'Processing Requirements']\n",
    "target_attribute = 'Execution Node'  # Update this if the actual column name is different\n",
    "decision_tree = create_decision_tree(df, attributes, target_attribute, max_depth=3, min_samples_split=5)\n",
    "\n",
    "# Print the decision tree (you might want to use a tree visualization library for a more readable display)\n",
    "print(decision_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6daba9-89ef-4f18-a578-c5fae1476a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
